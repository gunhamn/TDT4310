{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. Kochmar mentions several steps required in a typical NLP pipeline, one of them being *Split into words*. Why is this step necessary? Why can we not just feed the text as it is into a model?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- A full text will provide little to no meaning without properly discretizing the units of information\n",
    "    - e.g., words, multi-word expressions, ...\n",
    "- Predictability in modeling schemes: we cannot beforehand know the structure (such as the length) of a raw input text\n",
    "    - By tokenizing, we can impose rules such as \"maximum length 512 tokens\", and everything below this could for example get \"padded\" to ensure we always have the same input lengths\n",
    "- More advanced approaches like the attention mechanism relies on identfiying relationships between tokens\n",
    "\n",
    "The student is expected to at least reflect on some of these factors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. Simply splitting on \"words\" (i.e. whitespace) is rarely enough. Consider the sentence below (\"That U.S.A. poster-print costs $12.40...\") and name some problems that arise from splitting on whitespace.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The student is expected to know about certain issues on tokenization with symbols, abbreviations, capitalization, hyphenation, ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# If you wish, experiment with implementing different rules for tokenization. You will see that the \"ruleset\" quickly grows if you want to account for all types of edge cases...\n",
    "sentence = \"That U.S.A. poster-print costs $12.40...\"\n",
    "\n",
    "def your_rulebased_tokenizer(sentence):\n",
    "    tokens = []\n",
    "    return tokens\n",
    "\n",
    "your_rulebased_tokenizer(sentence)\n",
    "\n",
    "# if implemented, just make sure that the output is a list of tokens."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NLTK has several tokenizers implemented, such as a specific one for Twitter data. Below, indicated by the `TODO`-tag, you should find and import various tokenizers and add them to the list of tokenizers:\n",
    "\n",
    "`tokenizers = [tokenizer1, tokenizer2, ..., tokenizerN]`\n",
    "\n",
    "Tokenize the sentence with at least three different tokenizers supplied by NLTK and comment on your findings. You will find the documentation for NLTK's tokenizers [here](https://www.nltk.org/_modules/nltk/tokenize.html) useful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WhitespaceTokenizer (5 tokens)\n",
      "['That', 'U.S.A.', 'poster-print', 'costs', '$12.40...']\n",
      "\n",
      "TreebankWordTokenizer (7 tokens)\n",
      "['That', 'U.S.A.', 'poster-print', 'costs', '$', '12.40', '...']\n",
      "\n",
      "WordPunctTokenizer (16 tokens)\n",
      "['That', 'U', '.', 'S', '.', 'A', '.', 'poster', '-', 'print', 'costs', '$', '12', '.', '40', '...']\n",
      "\n",
      "TweetTokenizer (12 tokens)\n",
      "['That', 'U', '.', 'S', '.', 'A', '.', 'poster-print', 'costs', '$', '12.40', '...']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from typing import List\n",
    "\n",
    "from nltk.tokenize.api import TokenizerI\n",
    "from nltk.tokenize import WhitespaceTokenizer, TreebankWordTokenizer, WordPunctTokenizer, TweetTokenizer\n",
    "\n",
    "tokenizers = [\n",
    "    WhitespaceTokenizer(),\n",
    "    TreebankWordTokenizer(),  # tokenize according to the Penn Treebank\n",
    "    WordPunctTokenizer(),\n",
    "    TweetTokenizer()\n",
    "]\n",
    "# ************************************************************\n",
    "\n",
    "# Leave this function as-is\n",
    "def tokenize(tokenizers: List[TokenizerI], sentence: str) -> None:\n",
    "    for tokenizer in tokenizers:\n",
    "        assert isinstance(tokenizer, TokenizerI)\n",
    "        tokenized = tokenizer.tokenize(sentence)\n",
    "        print(f\"{tokenizer.__class__.__name__} ({len(tokenized)} tokens)\\n{tokenized}\\n\")\n",
    "\n",
    "\n",
    "tokenize(tokenizers, sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your comments on the outputs above here!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Language modeling\n",
    "We have now studied the bigger models like BERT and GPT-based language models. A simpler language model, however, can implemented using n-grams."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. What is an n-gram?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A sequence of n tokens (words, characters, ...) that occur within a text (in a sort of sliding window fashion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. Use NLTK to print out bigrams and trigrams for the given sentence below. Your function should support any number of N.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"That U.S.A. poster-print costs $12.40... I'd pay $5.00 for it.\"\n",
    "\n",
    "# ************************************\n",
    "# TODO: your implementation of n-grams\n",
    "# ************************************\n",
    "\n",
    "import nltk\n",
    "def get_ngram(sentence: str, n: int):\n",
    "    tokens = TreebankWordTokenizer().tokenize(sentence)\n",
    "    return list(nltk.ngrams(tokens, n))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3. Based on your intuition for language modeling, how can n-grams be used for word predictions?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A varying N could be used to create a simplistic language model that adheres specifically to the data it is trained on. If you train it on your own chats, for example, it would probably be alright. Using combinations of e.g. 2-grams, 3-grams and 4-grams could yield better results, if we weigh the probability of the next word based on the different contexts. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4. NLTK includes the `FreqDist` class, which produces the frequency distribution of words in a sentence. Use it to print out the two most common words in the text below.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('is', 5), ('that', 4)]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk import FreqDist\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "text = \"That that is is that that is not. Is that it? It is. You sure? Surely it is!\"\n",
    "tokens = word_tokenize(text)\n",
    "FreqDist(tokens).most_common(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**5. Use your n-gram function from question 2.2 to print out the most common trigram of the text in question 2.4**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('that', 'that', 'is'), 2)]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = text.lower()\n",
    "ngram = get_ngram(text, 3)\n",
    "ngram_freqs = FreqDist(ngram)\n",
    "ngram_freqs.most_common(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**6. You may have discovered that you would need to implement some form of preprocessing to get the correct answer to the previous tasks. Preprocessing/cleaning/normalization is often necessary for the desired results. If you were to process the text of a news site or blog post, can you think of some preprocessing steps that would be useful?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The student should answer something about:\n",
    "- issues with lowercasing (removing meaning from things like proper nouns, entities and more). Same with stopwords.\n",
    "- hyphenation (often occurring in dialogues and quotes)\n",
    "- symbols in general\n",
    "- emails, dates, URLs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Word Representations\n",
    "For more information on word representations, consult the lab description file and course material."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. Describe the main differences between bag-of-words and one-hot encoding through examples.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bag-of-words is a mapping/dictionary between the collection of words and their occurrences. \n",
    "\n",
    "Given an initial sentence like \"the cat and the dog\"\n",
    "\n",
    "The bag-of-words representation would be: \n",
    "```\n",
    "{\n",
    "    \"the\": 2,\n",
    "    \"cat\": 1,\n",
    "    \"and\": 1,\n",
    "    \"dog\": 1\n",
    "}\n",
    "```\n",
    "And thus, we can represent another sentence like \"the cat and the cat\" as a vector: [2, 2, 1, 0] and \"the cat\" as [1, 1, 0, 0].\n",
    "\n",
    "\n",
    "One-hot encoding is a binary representation of words in a vocabulary.\n",
    "With the same data as above, \"the cat and the dog\", the vocabulary would be represented as:\n",
    "```\n",
    "[\n",
    "    [1, 0, 0, 0],\n",
    "    [0, 1, 0, 0],\n",
    "    [0, 0, 1, 0],\n",
    "    [0, 0, 0, 1]\n",
    "]\n",
    "```\n",
    "The complete sentence is then `[1, 1, 1, 1]` and \"the cat\" would be `[1, 1, 0, 0]`. \"The cat and the cat\" would be `[1, 1, 1, 0]`.\n",
    "\n",
    "The main differences is that the one-hot encoding results in a binary (either/or) whether a word is present. Bag-of-words considers the frequency of the words within the document, whereas one-hot will not."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. What are the limitations of the above representations?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Positional data is not well represented with either BoW or one-hot encoding, and thus we lack semantic meaning compared to other alternatives like word embeddings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3. Example of word embedding techniques, such as Word2Vec and GloVe are considered *dense* representations. How do dense word embeddings relate to the *distributional hypothesis*?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The way dense representations are developed, such as skip-gram or CBoW, is by considering context when passed through the model. This directly relates to the distributional hypothesis, which states that words that occur in similar contexts are related in meaning. If a model is able to infer the correct word given its context (or vice versa), it has learned something about the distributional semantics of the involved words"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
