{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introductory exercise: Large language models (LLMs) and the latest developments\n",
    "Due to the immense impact of LLMs since the introduction of ChatGPT in November 2022, we encourage students to delve into topics related to LLMs before going into the basics of computational linguistics. Some suggested material is listed below, but you are expected to find reliable information from other sources as well to cover the questions.\n",
    "\n",
    "This exercise will consist of theoretical questions, where the answers should be around one sentence in length. Future exercises, or *labs*, will be more practical and hands-on.\n",
    "\n",
    "If you want to explore the use of these models, there's an added [`Playground`](#Playground) section at the very bottom of this file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Background: the transformer\n",
    "The transformer architecture is behind most of the advanced NLP models since its introduction in 2017 in the paper [Attention Is All You Need](https://proceedings.neurips.cc/paper_files/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf). It introduced parallelized self-attention mechanisms, allowing it to efficiently capture long-range dependencies in sequences, unlike RNNs (Recurrent Neural Networks) and LSTMs (Long Short-Term Memory networks). Two popular models based on the transformer architecture are BERT and GPT.\n",
    "\n",
    "### BERT \n",
    "[**BERT (Bidirectional Encoder Representations from Transformers)**](https://aclanthology.org/N19-1423.pdf) quickly gained popularity as a pre-trained model. BERT is trained for two main tasks: \n",
    "\n",
    "1. **Masked Language Model (MLM):** \n",
    "   - In the MLM task, BERT learns to predict masked tokens. For example: \n",
    "     - Original: \"The student is [MASK] about a new topic.\" \n",
    "     - Prediction: \"The student is curious about a new topic.\" \n",
    "\n",
    "2. **Next Sentence Prediction (NSP):** \n",
    "   - BERT is also trained to predict whether a randomly selected sentence logically follows another sentence in a document. For instance: \n",
    "     - Sentence 1: \"The cat is on the mat.\" \n",
    "     - Sentence 2: \"It is a comfortable spot for a nap.\" \n",
    "     - Random Sentence: \"The student loves reading.\" \n",
    "   - BERT learns that the random sentence is less likely to follow Sentence 1 or Sentence 2 in the document. \n",
    "\n",
    "A key feature of BERT is that it is bidirectional, meaning that it can use the context of a sentence to predict a masked token. Further, by fine-tuning it towards specific data, to be used for tasks like question answering and sentiment analysis.\n",
    "\n",
    "### GPT \n",
    "[GPT (Generative Pre-trained Transformer)](https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf) is essentially an autoregressive, decoder-only, language model. It learns to predict the next token given an existing series of tokens (e.g. predicting the next word in a sentence), left to right. This token is selected from a probability distribution of next tokens from its vocabulary. \n",
    "\n",
    "```\n",
    "Computer science students love to ___ \n",
    "- Code 0.90 \n",
    "- Study 0.09 \n",
    "- Cook 0.01 \n",
    "```\n",
    "\n",
    "## Suggested material\n",
    "- [Andrej Karpathy, 1hr talk on LLMs](https://www.youtube.com/watch?v=zjkBMFhNj_g)\n",
    "- [About ChatGPT](https://www.assemblyai.com/blog/how-chatgpt-actually-works/)\n",
    "- [Fine-tuning LLMs](https://www.analyticsvidhya.com/blog/2023/08/fine-tuning-large-language-models/)\n",
    "- Familiarize yourself with HuggingFace, a popular NLP library and model repository. They also host a [great blog](https://huggingface.co/blog) with many interesting articles on NLP."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. A language model (LM) is...**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. A large language model (LLM) is...**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3. A pre-trained base/foundation model is...**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4. What do we mean by fine-tuned language models, and what does the *head* mean in this context?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**5. Which one is more suitable for a chatbot: a pretrained model or a fine-tuned model?**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**6. What do we mean by knowledge cutoff in models like ChatGPT?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**7. What do we mean when we talk about confabulations/hallucinations of LLMs?**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**8. The context length (e.g. 4096) of a GPT-based model is...**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**9. Why is it problematic to increase the context/sequence length for transformer-based models?**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**10. Parameters are used to describe the model size of LLMs (such as 7B, 13B, â€¦). What do these parameters represent, and what makes LLMs difficult to interpret**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**11. Quantization (e.g. through techniques like LoRa, GPTQ, AWQ) is typically used to reduce the size of LLMs allow them to be run on consumer-grade hardware - even laptops! A common approach is to reduce the precision of the model weights. What does this mean?**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**12. LLMs need vast training data. The size of training data is typically referred to as tokens. What is a token and what happens to unknown words outside of the models' vocabulary?**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**13. What is the difference between model training and model inference?**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**14.  Why are GPUs (graphics processing units) ideal for training larger transformer-based models?**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**15.  What is the biggest technical limitation with respect to GPUs for training LLMs?**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**16. LLMs aimed towards human interactions should be *aligned*. What does this mean?**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**17. RLHF (reinforcement learning from human feedback) is used in models like ChatGPT. While you are not expected to learn about reinforcement learning (RL), explain the following terms in context of LLMs (1 sentence each):**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- SFT (supervised fine-tuning)\n",
    "    - your answer\n",
    "- Reward model\n",
    "    - your answer\n",
    "- Policy model\n",
    "    - your answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**18. For the above procedures, we require data, ideally labeled by humans. This is called data annotation. In this context, what is inter-annotator agreement, and why is it important for model creation?**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**19. Do you believe LLM projects should be open-sourced? Why**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**20. Why is it important to consider the ethical implications of LLMs**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Playground\n",
    "Below are a few examples you can run if you wish! These make use of the `transformers` library.\n",
    "The examples require around ~1 GB of available storage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# install requirements:\n",
    "%pip install transformers\n",
    "%pip install torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "model_id = \"EleutherAI/gpt-neo-125m\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id)\n",
    "\n",
    "print(f\"{model_id}'s vocab size is {model.config.vocab_size}\")\n",
    "\n",
    "query = \"We study because\"\n",
    "# this encodes the query according to the model's setup and vocabulary\n",
    "encoded_text = tokenizer(query, return_tensors=\"pt\")\n",
    "print(encoded_text)  # notice the input_ids, which are the token indices in the vocab\n",
    "# print a random vocab id:\n",
    "print(f\"Inspect a vocab id 1135: {tokenizer.decode(1135)}\")\n",
    "\n",
    "with torch.no_grad():  # disable gradient calculation for inference\n",
    "    outputs = model(**encoded_text)\n",
    "next_token = outputs.logits[-1, -1, :] # select the last token from the sequence and all the vocab logits (:)\n",
    "print(\"The outputs of the next token will be a distribution over the entire vocabulary\")\n",
    "print(f\"Output: {next_token.shape[0]} == Vocab: {model.config.vocab_size}\")\n",
    "\n",
    "probdist = torch.softmax(next_token, -1)  # -1 means last axis, or the vocab axis\n",
    "\n",
    "k_best = 5\n",
    "topk_next_tokens= torch.topk(probdist, k_best)\n",
    "\n",
    "print(\"-\" * 50)\n",
    "print(query)\n",
    "for idx, prob in zip(topk_next_tokens.indices, topk_next_tokens.values):\n",
    "    print(f\"- {tokenizer.decode(idx).strip()} (prob {prob.item():.4f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# it can also be used in a simple pipeline:\n",
    "from transformers import pipeline\n",
    "\n",
    "gpt_pipe = pipeline(\"text-generation\", model=\"EleutherAI/gpt-neo-125m\");\n",
    "output = gpt_pipe(\"This model is pretty bad, but\")\n",
    "print(output[0][\"generated_text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A BERT masked example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertForMaskedLM\n",
    "\n",
    "bert_model_id = \"bert-base-cased\"\n",
    "tokenizer = BertTokenizer.from_pretrained(bert_model_id)\n",
    "model = BertForMaskedLM.from_pretrained(bert_model_id)\n",
    "\n",
    "MASK = tokenizer.mask_token\n",
    "text = f\"At NTNU you can study {MASK} engineering\"\n",
    "\n",
    "encoded = tokenizer.encode(text)\n",
    "decoded = tokenizer.decode(encoded)\n",
    "tokens = tokenizer.tokenize(decoded)\n",
    "mask_position = tokens.index('[MASK]')\n",
    "\n",
    "input_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "input_ids = torch.tensor(input_ids).unsqueeze(0)\n",
    "\n",
    "with torch.no_grad():\n",
    "    predictions = model(input_ids)[0]\n",
    "\n",
    "top_k_values, top_k_indices = torch.topk(predictions[0, mask_position], k=10)\n",
    "top_k_tokens = tokenizer.convert_ids_to_tokens(top_k_indices)\n",
    "\n",
    "print(\"-\" * 50)\n",
    "print(text)\n",
    "for token, score in zip(top_k_tokens, top_k_values):\n",
    "    print(f\"{token} (prob: {score.item():.4f})\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
