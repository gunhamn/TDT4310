{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. Kochmar mentions several steps required in a typical NLP pipeline, one of them being *Split into words*. Why is this step necessary? Why can we not just feed the text as it is into a model?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your answer here!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. Simply splitting on \"words\" (i.e. whitespace) is rarely enough. Consider the sentence below (\"That U.S.A. poster-print costs $12.40...\") and name some problems that arise from splitting on whitespace.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your answer here!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# If you wish, experiment with implementing different rules for tokenization. You will see that the \"ruleset\" quickly grows if you want to account for all types of edge cases...\n",
    "sentence = \"That U.S.A. poster-print costs $12.40...\"\n",
    "\n",
    "def your_rulebased_tokenizer(sentence):\n",
    "    tokens = []\n",
    "    return tokens\n",
    "\n",
    "your_rulebased_tokenizer(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NLTK has several tokenizers implemented, such as a specific one for Twitter data. Below, indicated by the `TODO`-tag, you should find and import various tokenizers and add them to the list of tokenizers:\n",
    "\n",
    "`tokenizers = [tokenizer1, tokenizer2, ..., tokenizerN]`\n",
    "\n",
    "Tokenize the sentence with at least three different tokenizers supplied by NLTK and comment on your findings. You will find the documentation for NLTK's tokenizers [here](https://www.nltk.org/_modules/nltk/tokenize.html) useful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MyWhitespaceTokenizer (5 tokens)\n",
      "['That', 'U.S.A.', 'poster-print', 'costs', '$12.40...']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from typing import List\n",
    "\n",
    "# this is the base class of tokenizers in nltk\n",
    "from nltk.tokenize.api import TokenizerI\n",
    "\n",
    "\n",
    "# this is just a simple example of how a tokenizer can be implemented\n",
    "class MyWhitespaceTokenizer(TokenizerI):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def tokenize(self, text: str) -> List[str]:\n",
    "        return text.split()\n",
    "\n",
    "\n",
    "sentence = \"That U.S.A. poster-print costs $12.40...\"\n",
    "\n",
    "# ************************************************************\n",
    "# TODO: import and add the tokenizers you want to try out here\n",
    "# ************************************************************\n",
    "tokenizers = [\n",
    "    MyWhitespaceTokenizer(),\n",
    "]\n",
    "\n",
    "# Leave this function as-is\n",
    "def tokenize(tokenizers: List[TokenizerI], sentence: str) -> None:\n",
    "    for tokenizer in tokenizers:\n",
    "        assert isinstance(tokenizer, TokenizerI)\n",
    "        tokenized = tokenizer.tokenize(sentence)\n",
    "        print(f\"{tokenizer.__class__.__name__} ({len(tokenized)} tokens)\\n{tokenized}\\n\")\n",
    "\n",
    "\n",
    "tokenize(tokenizers, sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your comments on the outputs above here!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Language modeling\n",
    "We have now studied the bigger models like BERT and GPT-based language models. A simpler language model, however, can implemented using n-grams."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. What is an n-gram?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your answer here!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. Use NLTK to print out bigrams and trigrams for the given sentence below. Your function should support any number of N.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"That U.S.A. poster-print costs $12.40... I'd pay $5.00 for it.\"\n",
    "\n",
    "# ************************************\n",
    "# TODO: your implementation of n-grams\n",
    "# ************************************"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3. Based on your intuition for language modeling, how can n-grams be used for word predictions?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your answer here!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4. NLTK includes the `FreqDist` class, which produces the frequency distribution of words in a sentence. Use it to print out the two most common words in the text below.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**5. Use your n-gram function from question 2.2 to print out the most common trigram of the text in question 2.4**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**6. You may have discovered that you would need to implement some form of preprocessing to get the correct answer to the previous tasks. Preprocessing/cleaning/normalization is often necessary for the desired results. If you were to process the text of a news site or blog post, can you think of some preprocessing steps that would be useful?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your answer here!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Word Representations\n",
    "For more information on word representations, consult the lab description file and course material."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. Describe the main differences between bag-of-words and one-hot encoding through examples.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your answer here!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. What are the limitations of the above representations?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your answer here!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3. Example of word embedding techniques, such as Word2Vec and GloVe are considered *dense* representations. How do dense word embeddings relate to the *distributional hypothesis*?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your answer here!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
