{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 1\n",
    "### Published: Monday, Jan. 8, 2024\n",
    "### Deadline: Monday, Jan. 22, 2024\n",
    "\n",
    "Consult the [lab description](lab1_description.md) for background material and suggested reading.\n",
    "\n",
    "Many of these questions will likely feel challenging, as you have not yet been exposed to sufficient background material. They are, however, meant to cover both the curious student and introduce those new to the topics to the world of LLMs.\n",
    "\n",
    "## Requirements:\n",
    "\n",
    "- You must answer at least 10 out of the 21 questions for passing the lab.\n",
    "- Deliver a copy of this notebook on blackboard named `lab1_exercises_{your-username}.ipynb`\n",
    "\n",
    "## Questions:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. A language model (LM) is...**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is my answer =)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. A large language model (LLM) is...**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3. A pre-trained base/foundation model is...**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4. What do we mean by fine-tuned language models, and what does the *head* mean in this context?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**5. Which one is more suitable for a chatbot: a pretrained model or a pretrained + fine-tuned model?**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**6. What do we mean by knowledge cutoff in models like ChatGPT?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**7. What do we mean when we talk about confabulations/hallucinations of LLMs?**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**8. The context length (e.g. 4096) of a GPT-based model is...**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**9. Why is it problematic to increase the context/sequence length for transformer-based models?**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**10. Parameters are used to describe the model size of LLMs (such as 7B, 13B, â€¦). What do these parameters represent, and what makes LLMs difficult to interpret?**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**11. Quantization (e.g. through techniques like GPTQ and AWQ) is typically used to reduce the size of LLMs allow them to be run on consumer-grade hardware - even laptops! Explain the main idea behind quantization:**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**12. LLMs need vast training data. The size of training data is typically referred to as tokens. What is a token and what happens to unknown words outside of the models' vocabulary?**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**13. What is the difference between model training and model inference?**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**14.  Why are GPUs (graphics processing units) ideal for training larger transformer-based models?**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**15.  What is the biggest technical limitation with respect to GPUs for training LLMs?**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**16. LLMs aimed towards human interactions should be *aligned*. What does this mean?**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**17. RLHF (reinforcement learning from human feedback) is used in models like ChatGPT. While you are not expected to learn about reinforcement learning (RL), explain the following terms in context of LLMs (1 sentence each):**\n",
    "\n",
    "ðŸ”º this question might be too hard without any knowledge of RL. feel free to skip it. ðŸ”º"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- SFT (supervised fine-tuning)\n",
    "    - your answer\n",
    "- Reward model\n",
    "    - your answer\n",
    "- Policy model\n",
    "    - your answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**18. For the above procedures, we require data, ideally labeled by humans. This is called data annotation. In this context, what is inter-annotator agreement, and why is it important for model creation?**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**19. Do you believe LLM projects should be open-sourced? Why?**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**20. Why is it important to consider the ethical implications of LLMs?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**21. *Smaller* language models ($\\leq 100M$ parameters) will likely be important in the days to come. What are some motivations for smaller models?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feedback\n",
    "- **What are your expectations for this course? What do you hope to learn?**\n",
    "- **Were the questions too difficult?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your answer here"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
