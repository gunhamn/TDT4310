{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stemming and Lemmatization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Given the list of pluralized words below, define your own simple word stemmer function or class,  limited to only simple rules and regex. No libraries! It should strip basic endings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plurals = [\n",
    "    \"flies\",\n",
    "    \"denied\",\n",
    "    \"itemization\",\n",
    "    \"sensational\",\n",
    "    \"reference\",\n",
    "    \"colonizer\",\n",
    "]\n",
    "\n",
    "# TODO: implement your own simple stemmer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. After your initial implementation, run it on the following words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_words = [\n",
    "    \"friendly\",\n",
    "    \"puzzling\",\n",
    "    \"helpful\",\n",
    "]\n",
    "# TODO: run your stemmer on the new words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Realizing that fixing future words manually can be problematic, use a desired NLTK stemmer and run it on all the words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "all_words = plurals + new_words\n",
    "\n",
    "# TODO: use an nltk stemming implementation to stem `all_words`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. There are likely a few words in the outputs above that would cause issues in real-world applications. Pick some examples, and show how they are solved with a lemmatizer. Use either spaCy or nltk."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your answer here! Code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: basic observations on which examples are problematic with stemming + implement lemmatization with spacy/nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stemming/Lemmatization - Practical Example\n",
    "Using the news corpus (subset/category of the Brown corpus), perform common text normalization techniques such as stopword filtering and stemming/lemmatization. Compare the top 10 most common **words** before and after these normalization techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import nltk; nltk.download('brown')  # ensure we have the data\n",
    "from nltk.corpus import brown\n",
    "news = brown.words(categories='news')\n",
    "\n",
    "# TODO: find the top 10 most common words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: find the top 10 most common words after applying text normalization techniques"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF-IDF\n",
    "TF-IDF (term frequency-inverse document frequency) is a way to measure the importance of a word in a document.\n",
    "\n",
    "$$\n",
    "\\text{tf-idf}(t, d, D) = \\text{tf}(t, d) \\times \\text{idf}(t, D)\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $t$ is the term (word)\n",
    "- $d$ is the document\n",
    "- $D$ is the corpus\n",
    "\n",
    "\n",
    "\n",
    "#### 1. Implement TF-IDF using NLTKs FreqDist (no use of e.g. scikit-learn and other high-level libraries)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "\n",
    "##########################################################\n",
    "# Feel free to change everything below.\n",
    "# It is merely a guide to understand the inputs/outputs\n",
    "##########################################################\n",
    "\n",
    "\n",
    "############ TODO ############\n",
    "def tf(document: List[str], term: str) -> float:\n",
    "    \"\"\"\n",
    "    Calculate the term frequency (TF) of a given term in a document.\n",
    "\n",
    "    Args:\n",
    "        document (List[str]): The document in which to calculate the term frequency.\n",
    "        term (str): The term for which to calculate the term frequency.\n",
    "\n",
    "    Returns:\n",
    "        float: The term frequency of the given term in the document.\n",
    "    \"\"\"\n",
    "    return\n",
    "\n",
    "\n",
    "############ TODO ############\n",
    "def idf(documents: List[List[str]], term: str) -> float:\n",
    "    \"\"\"\n",
    "    Calculate the inverse document frequency (IDF) of a term in a collection of documents.\n",
    "\n",
    "    Args:\n",
    "        documents (List[List[str]]): A list of documents, where each document is represented as a list of strings.\n",
    "        term (str): The term for which IDF is calculated.\n",
    "\n",
    "    Returns:\n",
    "        float: The IDF value of the term.\n",
    "    \"\"\"\n",
    "    return\n",
    "\n",
    "\n",
    "############ TODO ############\n",
    "def tf_idf(\n",
    "    all_documents: List[List[str]],\n",
    "    document: List[str],\n",
    "    term: str,\n",
    ") -> float:\n",
    "    return\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. With your TF-IDF function in place, calculate the TF-IDF for the following words in the first document of the news articles found in the Brown corpus: \n",
    "\n",
    "- *the*\n",
    "- *nevertheless*\n",
    "- *highway*\n",
    "- *election*\n",
    "\n",
    "Perform any preprocessing steps you deem necessary. Comment on your findings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fileids = brown.fileids(categories='news')\n",
    "first_doc = list(brown.words(fileids[0]))\n",
    "all_docs = [list(brown.words(fileid)) for fileid in fileids]\n",
    "\n",
    "# TODO: preprocess and calculate tf-idf scores."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. While TF-IDF is primarily used for information retrieval and text mining, reflect on how TF-IDF could be used in a language modeling context."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your answer here!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. You were previously introduced to word representations. TF-IDF can be considered one. What are some differences between the TF-IDF output and one that is computed once from a vocabulary (e.g. one-hot encoding)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your answer here!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF-IDF - Practical Example\n",
    "You will again be looking at specific words for a document, but this time weighted by their TF-IDF scores. Ideally, the scoring should be able to retrieve representative words for this document in context of its document collection or category.\n",
    "\n",
    "You will do the following:\n",
    "- Select a category from the Reuters (news) corpus\n",
    "- Perform preprocessing\n",
    "- Calculate TF-IDF scores\n",
    "- Find the top 5 words for *each document* in a subset of documents in your collection (e.g. 5, 10, ... documents total)\n",
    "- Inspect whether these words make sense for a given document, and comment on your findings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk; nltk.download(\"reuters\")\n",
    "from nltk.corpus import reuters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part-of-speech tagging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Briefly describe your understanding of POS tagging and its possible use-cases in context of text generation applications/language modeling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your answer here!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Train a UnigramTagger (NLTK) using the Brown corpus. \n",
    "Hint: the taggers in nltk require a list of sentences containing tagged words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: train a unigram tagger on the brown corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Use this tagger to tag the text given below. Print out the POS tags for all variants of \"justify\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"\n",
    "Imagine a situation where you have to explain why you did something â€“ that's when you justify your actions. So, let's say you made a decision; you, as the justifier, need to give good reasons (justifications) for your choice. You might use justifying words to make your point clear and reasonable. Justifying can be a bit like saying, \"Here's why I did what I did.\" When you justify things, you're basically providing the why behind your actions. So, being a good justifier involves carefully explaining, giving reasons, and making sure others understand your choices\n",
    "\"\"\"\n",
    "\n",
    "# TODO: use your trained tagger"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Your results may be disappointing. Repeat the same task as above using both the default NLTK pos-tagger and with spaCy. Compare the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: use the default NLTK tagger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: use spacy to fetch pos tags from the document"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. Finally, explore more features of the what the spaCy *document* includes related to topics covered in this lab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
