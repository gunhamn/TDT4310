{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 3\n",
    "This time we will do more manipulation of language through stemming/lemmatization, and then move on to TF-IDF (term frequency-inverse document frequency).\n",
    "\n",
    "Finally, you will work on part-of-speech (POS) tagging, which is the process of marking up a word in a text (corpus) as corresponding to a particular part of speech (e.g., noun, adjective, verb, ...), based on both its definition and its context. \n",
    "\n",
    "## Libraries\n",
    "We continue to use NLTK, but we now also add spaCy to our repotoire, which will be used to explore POS tags and more. See https://spacy.io/. Feel free to experiment more with the many features of spaCy! They have recently started adding support for LLM applications as well.\n",
    "\n",
    "## Installation:\n",
    "```bash\n",
    "pip install -U spacy\n",
    "python -m spacy download en_core_web_sm\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading material\n",
    "\n",
    "Some previous chapters (in Kochmar) were set up as suggested reading in Lab 2 to prepare you for this one, namely the concepts around stemming, lemmatization, and TF-IDF. To follow along with the book, we still advise you to skim through chapters 4, 5 and 6. Chapter 4 shows good practical examples of spaCy for POS tagging and lemmatization. Chapters 5 and 6 are more focused towards the machine learning aspects of this course.\n",
    "\n",
    "\n",
    "\n",
    "NLTK book chapters 2 and 3 are still relevant. Chapter 4 explains some suggested programming styles with Python. Chapter 5 is a good introduction to part-of-speech tagging. Chapter 6 starts with basic foundations to supervised learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Normalization\n",
    "Text normalization deals with a large set of techniques, but most commonly:\n",
    "\n",
    "- Dealing with unwanted characters and symbols (e.g. keeping only alphanumeric characters)\n",
    "- Lower/uppercasing\n",
    "- Stopword filtering\n",
    "- Stemming and lemmatization\n",
    "\n",
    "The last point is the task of converting text to its standard form. While stemming is a simpler process of reducing words to a common core (*stem*), often by removing suffixes, lemmatization will use dictionaries and definitions to convert words to their base form. The differences can be illustrated as follows:\n",
    "\n",
    "| Word | Stemming | Lemmatization |\n",
    "| - | - | - |\n",
    "| information | inform | information\n",
    "| informative | inform | informative\n",
    "| computers | comput | computer\n",
    "| feet | feet | foot\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In chapter 3, Kochmar motivated the use of these concepts by an example:\n",
    "\n",
    "> \"Imagine you wanted to know what *sung* meant. Your best strategy would be to look up sing. Similarly, the search algorithm would benefit from mapping sing, sang, and sung to the same word form, by default the most basic one - sing.\n",
    "\n",
    "This property is useful for information search and extraction, especially when dealing with morphologically rich languages."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TF-IDF and POS-tagging are two topics covered well by course book(s) and the lectures."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
